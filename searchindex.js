Search.setIndex({"docnames": ["algos/algorithms", "algos/multi_policy", "algos/multi_policy/envelope", "algos/multi_policy/gpi_pd", "algos/multi_policy/linear_support", "algos/multi_policy/mp_mo_q_learning", "algos/multi_policy/pareto_q_learning", "algos/multi_policy/pcn", "algos/multi_policy/pgmorl", "algos/performances", "algos/single_policy", "algos/single_policy/eupg", "algos/single_policy/moq_learning", "community/community", "features/buffers", "features/evaluations", "features/misc", "features/networks", "features/pareto", "features/performance_indicators", "features/scalarization", "index", "quickstart/overview"], "filenames": ["algos/algorithms.md", "algos/multi_policy.md", "algos/multi_policy/envelope.md", "algos/multi_policy/gpi_pd.md", "algos/multi_policy/linear_support.md", "algos/multi_policy/mp_mo_q_learning.md", "algos/multi_policy/pareto_q_learning.md", "algos/multi_policy/pcn.md", "algos/multi_policy/pgmorl.md", "algos/performances.md", "algos/single_policy.md", "algos/single_policy/eupg.md", "algos/single_policy/moq_learning.md", "community/community.md", "features/buffers.md", "features/evaluations.md", "features/misc.md", "features/networks.md", "features/pareto.md", "features/performance_indicators.md", "features/scalarization.md", "index.md", "quickstart/overview.md"], "titles": ["Overview", "Multi-Policy Algorithms", "Envelope Q-Learning", "GPI-Prioritized Dyna", "Linear Support", "MPMOQ Learning", "Pareto Q-Learning", "Pareto Conditioned Networks", "PGMORL", "Performance assessments", "Single-policy Algorithms", "EUPG", "MOQ-Learning", "Community", "Replay Buffers", "Evaluations", "Miscellaneous", "Neural Networks helpers", "Pareto utils", "Performance indicators", "Scalarization functions", "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.", "Overview"], "terms": {"morl": [0, 2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 16, 19, 22], "baselin": [0, 2, 3, 5, 6, 7, 8, 11, 12, 13, 16, 22], "contain": [0, 14, 16, 21, 22], "multipl": [0, 2, 8, 14, 15], "implement": [0, 4, 8, 9, 13, 14, 21, 22], "multi": [0, 2, 3, 5, 6, 8, 11, 12, 16, 17, 19, 22], "object": [0, 2, 3, 4, 5, 6, 8, 9, 11, 12, 16, 19], "reinforc": [0, 2, 5, 6, 8, 9, 11, 12, 14, 17], "learn": [0, 3, 8, 9, 11, 13, 14, 17], "algorithm": [0, 2, 3, 4, 5, 8, 11, 12, 13, 14, 19, 22], "The": [0, 2, 4, 5, 6, 8, 9, 11, 14, 16, 17, 18, 21, 22], "follow": [0, 5, 9, 14, 16, 17, 21, 22], "tabl": [0, 9, 12], "list": [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 14, 16, 17, 19], "ar": [0, 4, 8, 9, 13, 14, 16, 18, 19, 21], "current": [0, 2, 4, 6, 8, 9, 13, 14, 16, 17, 19, 21], "name": [0, 6], "singl": [0, 12, 21, 22], "polici": [0, 2, 3, 4, 5, 6, 7, 8, 11, 12, 15, 16, 19, 21, 22], "esr": [0, 11, 14, 15, 21, 22], "ser": [0, 8, 12, 21, 22], "observ": [0, 2, 7, 8, 11, 12, 14, 17], "space": [0, 8, 9, 16], "action": [0, 2, 3, 5, 6, 7, 8, 11, 12, 14], "paper": [0, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 17, 19, 21], "gpi": [0, 4, 5], "l": [0, 3, 4, 9, 19], "pd": [0, 3], "continu": [0, 8, 9], "discret": 0, "supplementari": [0, 8], "materi": [0, 8], "envelop": 0, "q": [0, 5, 12, 13], "pgmorl": [0, 9, 19], "1": [0, 2, 3, 4, 5, 6, 7, 8, 9, 12, 14, 16], "pareto": [0, 2, 3, 5, 8, 9, 13, 16, 19, 21], "condit": [0, 2, 11, 15], "network": [0, 2, 3, 8, 11, 16], "pcn": [0, 7], "2": [0, 3, 8, 9, 14], "mo": [0, 5, 9, 12, 15, 21, 22], "mpmoqlearn": [0, 5], "outer": [0, 5], "loop": [0, 5], "moql": 0, "optimist": [0, 4], "linear": 0, "support": [0, 3, 8, 9], "ol": [0, 4], "section": [0, 4, 8], "3": [0, 4, 6, 8, 9, 21], "thesi": [0, 4], "expect": [0, 9, 11, 16, 19], "util": [0, 4, 9, 11, 12, 15, 16, 17, 19, 21], "gradient": [0, 11], "eupg": [0, 13], "warn": [0, 9], "have": [0, 8, 9, 13, 16, 18, 21], "been": [0, 8, 16, 18, 21], "benchmark": 0, "yet": [0, 8], "some": [0, 8, 9, 19], "them": [0, 9, 13], "limit": 0, "featur": [0, 13, 17], "i": [0, 2, 4, 5, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], "environ": [0, 2, 3, 4, 5, 6, 7, 9, 11, 12, 15, 21, 22], "assum": 0, "determinist": 0, "transit": [0, 14], "class": [2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 17, 18], "morl_baselin": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21], "multi_polici": [2, 3, 4, 5, 6, 7, 8, 22], "env": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16], "learning_r": [2, 3, 5, 7, 8, 11, 12], "float": [2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 15, 16, 17, 19, 20], "0": [2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 16, 17], "0003": [2, 3, 8], "initial_epsilon": [2, 3, 5, 6, 12], "01": [2, 3, 7, 14, 16], "final_epsilon": [2, 3, 5, 6, 12], "epsilon_decay_step": [2, 3, 5, 6, 12], "int": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 19, 20], "none": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16], "tau": [2, 3, 16, 20], "target_net_update_freq": [2, 3], "1000": [2, 3, 5, 6, 11, 12], "buffer_s": [2, 3, 11], "1000000": [2, 3], "net_arch": [2, 3, 8, 11, 17], "256": [2, 3], "batch_siz": [2, 3, 7, 14], "learning_start": [2, 3, 12], "100": [2, 3, 5, 7, 8], "gradient_upd": [2, 3], "gamma": [2, 3, 5, 6, 7, 8, 11, 12], "99": [2, 3, 11], "max_grad_norm": [2, 3, 8], "bool": [2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 15, 16, 17, 19], "true": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 16, 19], "num_sample_w": 2, "4": [2, 8], "per": [2, 3, 5, 6, 7, 12, 16, 17], "per_alpha": 2, "6": [2, 3, 8, 12], "initial_homotopy_lambda": 2, "final_homotopy_lambda": 2, "homotopy_decay_step": 2, "project_nam": [2, 3, 5, 6, 7, 8, 11, 12], "str": [2, 3, 4, 5, 6, 7, 8, 11, 12, 16, 19], "experiment_nam": [2, 3, 5, 6, 7, 8, 11, 12], "wandb_ent": [2, 3, 5, 6, 7, 8, 11, 12], "log": [2, 3, 5, 6, 7, 8, 9, 11, 12, 16], "seed": [2, 3, 5, 6, 7, 8, 9, 11, 12, 16], "devic": [2, 3, 7, 8, 11, 14], "auto": [2, 3, 7, 8, 11], "lean": 2, "us": [2, 3, 4, 5, 6, 8, 9, 11, 14, 16, 17, 19, 22], "emb": 2, "take": [2, 15], "weight": [2, 3, 4, 5, 7, 9, 11, 12, 15, 16, 19, 20, 21], "input": [2, 16, 17, 18], "main": [2, 8, 13, 14, 16], "chang": [2, 3, 8, 9, 18], "thi": [2, 7, 8, 9, 13, 14, 15, 16, 17, 18, 20, 21, 22], "compar": 2, "scalar": [2, 4, 5, 8, 9, 11, 12, 15, 16], "cn": 2, "dqn": [2, 16, 17], "target": [2, 14, 16], "updat": [2, 3, 7, 8, 11, 12, 14, 16], "r": [2, 19], "yang": 2, "x": [2, 16], "sun": 2, "k": [2, 5, 6, 12], "narasimhan": 2, "A": [2, 5, 6, 7, 8, 9, 11, 12, 14, 16, 19], "gener": [2, 3, 4, 5, 9, 11, 12, 16, 19], "adapt": [2, 8, 20], "arxiv": [2, 3, 4], "1908": 2, "08342": 2, "c": [2, 3, 9], "nov": [2, 8, 9], "2019": 2, "access": 2, "sep": 2, "06": 2, "2021": 2, "onlin": 2, "avail": [2, 7, 8, 9, 14, 21], "http": [2, 3, 4, 7, 8, 9, 14, 16, 18, 21], "org": [2, 3, 4, 7, 16], "ab": [2, 3, 4], "act": 2, "ob": [2, 3, 4, 5, 7, 8, 11, 12, 14], "tensor": [2, 3, 14, 16, 17], "w": [2, 3, 4, 5, 7, 8, 9, 11, 12, 15], "epsilon": [2, 4, 16], "greedili": 2, "select": [2, 3, 6, 8], "an": [2, 3, 4, 6, 9, 13, 16, 17], "given": [2, 3, 5, 6, 7, 8, 11, 12, 14, 16], "paramet": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21], "vector": [2, 3, 4, 6, 8, 15, 16, 17, 18, 19, 20], "return": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 18, 19, 20, 21], "integ": 2, "repres": [2, 17], "ddqn_target": 2, "doubl": 2, "envelope_target": 2, "sampled_w": 2, "comput": [2, 4, 6, 8, 9, 14, 16, 19], "set": [2, 3, 4, 5, 6, 7, 9, 14, 16, 18, 19, 22], "sampl": [2, 3, 8, 9, 14, 16], "eval": [2, 3, 5, 7, 8, 9, 11, 12], "ndarrai": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 15, 16, 17, 18, 19, 20], "give": [2, 11, 12, 14], "best": [2, 5, 8, 11, 12, 20], "np": [2, 3, 4, 11, 12, 15, 16, 19], "arrai": [2, 5, 8, 11, 12, 14, 16, 18, 21], "option": [2, 3, 4, 6, 11, 12, 15], "get_config": [2, 3, 5, 6, 7, 8, 11, 12], "dictionari": [2, 5, 6, 8, 11, 12, 14, 16], "configur": [2, 3, 5, 6, 7, 8, 9, 11, 12], "dict": [2, 5, 6, 7, 8, 11, 12, 16], "config": [2, 5, 8, 11, 12], "load": [2, 3], "path": [2, 3], "load_replay_buff": [2, 3], "model": [2, 3, 7, 12], "replai": [2, 3, 7, 22], "buffer": [2, 3, 7, 8, 11, 21, 22], "specifi": [2, 14], "whether": [2, 3, 12, 14, 15, 16, 17], "too": 2, "max_act": [2, 3], "highest": [2, 4], "valu": [2, 4, 5, 6, 8, 9, 12, 14, 15, 16, 19, 20], "save": [2, 3, 7, 16], "save_replay_buff": [2, 3], "save_dir": [2, 3], "filenam": [2, 3, 7], "directori": 2, "train": [2, 3, 5, 6, 7, 8, 9, 11, 12, 16], "total_timestep": [2, 3, 5, 6, 7, 8, 11, 12], "eval_env": [2, 3, 5, 6, 7, 8, 11, 12], "ref_point": [2, 3, 5, 6, 7, 8, 19], "known_pareto_front": [2, 3, 5, 6, 7, 8], "total_episod": 2, "reset_num_timestep": [2, 3, 12], "eval_freq": [2, 3, 5, 11, 12], "10000": [2, 3, 6], "num_eval_weights_for_front": [2, 3, 5], "num_eval_episodes_for_front": [2, 3, 5], "5": [2, 3, 5, 8, 12, 15], "reset_learning_start": [2, 3], "fals": [2, 3, 4, 5, 8, 12, 14, 15, 17], "agent": [2, 3, 4, 7, 8, 9, 11, 12, 15, 16], "total": [2, 5, 7], "number": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17], "timestep": [2, 3, 5, 6, 11, 12, 16], "evalu": [2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 16, 18], "If": [2, 4, 5, 6, 9, 13, 14, 16], "ignor": 2, "refer": [2, 3, 5, 6, 7, 8, 16, 19, 20], "point": [2, 3, 5, 6, 7, 9, 16, 18, 19, 20], "hypervolum": [2, 3, 5, 6, 7, 9, 16, 19], "known": [2, 3, 5, 6, 7, 9, 16, 19], "front": [2, 3, 5, 6, 7, 9, 16, 18, 19], "randomli": 2, "everi": [2, 6], "episod": [2, 3, 5, 6, 7, 11, 15, 16], "done": [2, 14], "reset": [2, 3, 12], "when": [2, 3, 4, 5, 8, 9, 12, 14], "time": [2, 4, 7, 8, 12], "frequenc": [2, 5, 11], "step": [2, 7, 9, 16], "creat": [2, 17], "run": [2, 3, 5, 9, 11, 15, 21], "start": [2, 3, 8, 12, 14], "": [2, 8, 9, 11, 14, 16, 21, 22], "e": [2, 7, 8, 11, 14, 15, 19, 21], "g": [2, 8, 11, 19, 21], "experi": [2, 7, 8, 11, 14, 21], "from": [2, 4, 6, 7, 8, 9, 11, 13, 14, 16, 17, 19, 21], "gpi_pd": [3, 5, 12], "gpipd": 3, "type": [3, 17, 19], "num_net": 3, "use_gpi": [3, 5], "alpha_p": 3, "min_prior": [3, 12, 14, 16], "drop_rat": [3, 17], "layer_norm": [3, 17], "dynamics_normalize_input": 3, "dynamics_uncertainty_threshold": 3, "dynamics_train_freq": 3, "callabl": [3, 6, 19, 20], "function": [3, 4, 5, 6, 8, 9, 12, 14, 15, 16, 17, 18, 19, 21], "lambda": [3, 14], "dynamics_rollout_len": 3, "dynamics_rollout_start": 3, "5000": 3, "dynamics_rollout_freq": 3, "250": [3, 7], "dynamics_rollout_batch_s": 3, "dynamics_buffer_s": 3, "400000": 3, "dynamics_net_arch": 3, "200": 3, "dynamics_ensemble_s": 3, "dynamics_num_elit": 3, "real_ratio": 3, "05": [3, 14], "torch": [3, 12, 16, 17], "effici": 3, "via": 3, "improv": [3, 4], "luca": [3, 13, 21], "n": [3, 7, 13, 14, 16, 21], "alegr": [3, 13, 21], "ana": 3, "bazzan": 3, "diederik": 3, "m": [3, 5, 7, 9, 12, 19], "roijer": [3, 4, 9, 11, 19], "ann": 3, "now\u00e9": [3, 6, 7], "bruno": 3, "da": 3, "silva": 3, "aama": 3, "2023": 3, "2301": [3, 4], "07784": [3, 4], "gpi_act": 3, "return_policy_index": 3, "include_w": 3, "greedi": [3, 12], "set_weight_support": 3, "weight_list": 3, "timesteps_per_it": 3, "weight_selection_algo": [3, 5], "gym": [3, 6], "calcul": [3, 5, 7, 18], "optim": [3, 4, 5, 6, 7, 9, 19, 21], "iter": [3, 5, 8, 16], "train_iter": 3, "weight_support": 3, "change_w_every_episod": 3, "one": [3, 12, 15, 16, 19], "end": [3, 4, 14], "each": [3, 8, 9, 12, 14, 17, 19, 20], "between": [3, 9, 12, 19], "linear_support": 4, "linearsupport": 4, "num_object": 4, "verbos": [4, 16], "corner": 4, "both": [4, 9, 21], "info": [4, 16], "pub": 4, "pdf": [4, 7, 8], "add_solut": 4, "add": [4, 8, 14, 18], "new": [4, 8, 13, 14], "indic": [4, 5, 14, 18], "remov": [4, 14, 18], "cc": [4, 9], "being": 4, "domin": [4, 6, 18, 20], "compute_corner_weight": 4, "see": [4, 8, 22], "definit": [4, 21], "19": 4, "typo": 4, "sign": 4, "should": [4, 14, 16], "queue": 4, "empti": 4, "get_corner_weight": 4, "top_k": 4, "get_weight_support": 4, "gpi_ls_prior": 4, "gpi_expanded_set": 4, "get": [4, 5, 6, 7, 13, 14], "prioriti": [4, 14, 16], "is_domin": 4, "check": [4, 14], "ani": [4, 19], "otherwis": [4, 5], "max_scalarized_valu": 4, "maximum": [4, 5, 7, 8, 9, 14, 16, 19], "max_value_lp": 4, "w_new": 4, "upper": 4, "bound": 4, "next_weight": 4, "algo": 4, "gpi_ag": 4, "mopolici": 4, "rep_ev": 4, "next": [4, 8, 14], "either": [4, 16], "ols_prior": 4, "remove_obsolete_valu": 4, "which": [4, 9, 14, 16, 17, 18, 21], "longer": 4, "after": [4, 8, 17], "ad": 4, "remove_obsolete_weight": 4, "new_valu": 4, "better": 4, "than": [4, 8], "previou": [4, 14], "multi_policy_moqlearn": 5, "mp_mo_q_learn": 5, "weighted_sum": [5, 12, 20], "9": [5, 12], "random": [5, 12, 16], "epsilon_ol": 5, "use_gpi_polici": [5, 12], "transfer_q_t": 5, "dyna": [5, 12], "dyna_upd": [5, 12], "multipolici": 5, "moq": 5, "version": 5, "mo_q_learn": [5, 12], "van": [5, 6, 12], "moffaert": [5, 6, 12], "drugan": [5, 12], "now": [5, 8, 11, 12], "novel": [5, 12], "design": [5, 12], "techniqu": [5, 12], "2013": [5, 12], "doi": [5, 9, 12], "10": [5, 7, 8, 9, 12], "1109": [5, 12], "adprl": [5, 12], "6615007": [5, 12], "delete_polici": 5, "delete_indx": 5, "delet": 5, "choos": [5, 12], "max_scalar_q_valu": 5, "state": [5, 6, 16, 19], "over": [5, 16], "all": [5, 9, 13, 14, 16, 18, 21, 22], "timesteps_per_iter": 5, "200000": 5, "metric": [5, 6, 7, 16, 19], "construct": 5, "pareto_q_learn": 6, "pql": 6, "8": 6, "100000": [6, 11, 14], "tabular": 6, "method": [6, 9, 14, 16], "reli": [6, 8, 9, 12, 19, 20], "prune": [6, 21], "journal": [6, 21], "machin": [6, 8, 9], "research": 6, "vol": [6, 9], "15": 6, "pp": [6, 7, 8, 9], "3483": 6, "3512": 6, "2014": 6, "calc_non_domin": 6, "non": [6, 18], "get_local_pc": 6, "collect": [6, 8, 17], "local": 6, "pc": 6, "default": [6, 9, 14, 15, 16, 19], "get_q_set": 6, "pair": [6, 14], "score_hypervolum": 6, "score": 6, "base": [6, 8, 9, 11, 19], "upon": 6, "score_pareto_cardin": 6, "cardin": 6, "select_act": 6, "score_func": 6, "track_polici": 6, "vec": 6, "tol": [6, 16], "001": [6, 11], "track": [6, 9, 21], "its": [6, 14, 19], "array_lik": 6, "toler": [6, 16], "1e": [6, 14], "log_everi": 6, "action_ev": 6, "eval_ref_point": 6, "dure": [6, 8], "same": [6, 14], "ref": 6, "result": [6, 9, 21], "final": [6, 9, 16], "scaling_factor": 7, "32": [7, 8], "hidden_dim": 7, "64": [7, 8], "reymond": 7, "bargiacchi": 7, "2022": [7, 9, 21], "mai": 7, "In": [7, 9], "proceed": [7, 8, 9], "21st": 7, "intern": [7, 8, 9], "confer": [7, 8, 9], "autonom": [7, 9], "multiag": 7, "system": [7, 9], "1110": 7, "1118": 7, "www": 7, "ifaama": 7, "aamas2022": 7, "p1110": 7, "credit": 7, "code": [7, 8, 14, 18, 21], "refactor": [7, 8], "author": [7, 8, 21], "github": [7, 8, 14, 21], "com": [7, 8, 14, 18, 21], "mathieu": 7, "max_return": 7, "pcn_model": 7, "savedir": 7, "set_desired_return_and_horizon": 7, "desired_return": 7, "desired_horizon": 7, "desir": 7, "horizon": 7, "num_er_episod": 7, "500": [7, 8], "num_step_episod": 7, "num_model_upd": 7, "max_buffer_s": 7, "fill": [7, 14, 16], "clip": 7, "size": [7, 14, 16], "ha": [8, 16], "origin": [8, 9, 13, 21], "provid": [8, 9, 13, 14, 16, 18, 21], "post": 8, "process": [8, 16], "phase": 8, "analysi": [8, 9], "stage": 8, "ppo": 8, "look": 8, "variou": [8, 9, 13, 19, 21], "tradeoff": 8, "keep": 8, "popul": 8, "along": [8, 9], "perform": [8, 21], "At": 8, "few": 8, "assign": 8, "further": 8, "histor": 8, "data": [8, 14], "gather": 8, "our": [8, 9, 13, 21], "essenti": 8, "cleanrl": [8, 21], "differ": [8, 21], "sum": [8, 20], "note": 8, "might": 8, "possibl": [8, 9, 22], "enhanc": 8, "someth": 8, "els": 8, "single_polici": [8, 11, 12, 22], "mo_ppo": 8, "id": [8, 12, 14, 16], "mopponet": 8, "syncvectorenv": 8, "writer": [8, 9, 12, 16], "summarywrit": [8, 9, 12, 16], "steps_per_iter": 8, "2048": 8, "num_minibatch": 8, "update_epoch": 8, "995": 8, "anneal_lr": 8, "clip_coef": 8, "ent_coef": 8, "vf_coef": 8, "clip_vloss": 8, "norm_adv": 8, "target_kl": 8, "gae": 8, "gae_lambda": 8, "95": 8, "42": [8, 16], "rng": [8, 16], "modifi": 8, "net": [8, 17, 22], "appli": [8, 14], "clean": 8, "rl": [8, 19, 21], "vwxyzjn": 8, "blob": 8, "master": 8, "ppo_continuous_act": 8, "py": [8, 9], "change_weight": 8, "new_weight": 8, "numpi": [8, 12, 14, 15, 19, 20, 21], "start_tim": [8, 12], "current_iter": 8, "max_iter": 8, "self": 8, "num_env": 8, "more": [8, 22], "detail": [8, 21, 22], "performancepredictor": 8, "neighborhood_threshold": 8, "sigma": 8, "03": 8, "a_bound_min": 8, "a_bound_max": 8, "f_scale": 8, "20": 8, "store": [8, 14], "delta": 8, "Then": 8, "regress": 8, "eval_before_pg": 8, "eval_after_pg": 8, "predictor": 8, "befor": [8, 16], "predict_next_evalu": 8, "weight_candid": 8, "policy_ev": 8, "tupl": [8, 14, 15], "part": 8, "determin": [8, 14], "neighborhood": 8, "threshold": 8, "whose": [8, 14], "candid": [8, 18], "env_id": 8, "pop_siz": 8, "warmup_iter": 8, "80": 8, "evolutionary_iter": 8, "num_weight_candid": 8, "7": 8, "num_performance_buff": 8, "performance_buffer_s": 8, "min_weight": 8, "max_weight": 8, "delta_weight": 8, "guid": [8, 9, 21], "j": [8, 9], "xu": [8, 9], "y": [8, 9], "tian": [8, 9], "p": [8, 9, 19], "ma": [8, 9], "d": [8, 9, 11, 19], "ru": [8, 9], "sueda": [8, 9], "matusik": [8, 9], "robot": [8, 9], "control": [8, 9, 17], "37th": [8, 9], "2020": [8, 9], "10607": [8, 9], "10616": [8, 9], "mlr": [8, 9], "press": [8, 9], "v119": [8, 9], "xu20h": [8, 9], "html": [8, 9, 16], "peopl": [8, 13], "csail": 8, "mit": 8, "edu": 8, "jiex": 8, "supp": 8, "document": [9, 21, 22], "work": 9, "progress": 9, "To": 9, "ensur": 9, "correct": 9, "we": [9, 13, 14, 19, 21], "want": [9, 13], "test": [9, 21], "For": [9, 21], "sake": 9, "reproduc": [9, 16], "mainten": 9, "purpos": 9, "long": 9, "term": 9, "conduct": 9, "gymnasium": [9, 15, 21, 22], "henc": 9, "abl": 9, "were": 9, "present": 9, "keyword": 9, "scalarized_return": 9, "scalarized_discounted_return": 9, "propos": 9, "qualiti": [9, 19], "discount": [9, 15], "pf": [9, 19], "convex": 9, "coverag": 9, "converg": 9, "divers": 9, "hybrid": 9, "common": [9, 12, 14, 15, 16, 17, 18, 19, 20, 22], "performance_ind": [9, 19], "sparsiti": [9, 16, 19], "averag": [9, 15, 16, 19], "distanc": [9, 14, 16, 19], "consecut": 9, "igd": [9, 16, 19], "sota": 9, "moo": [9, 19], "literatur": 9, "It": [9, 16, 17, 20, 21], "requir": [9, 19, 20], "can": [9, 13, 21], "posteriori": 9, "That": [9, 16], "do": 9, "merg": 9, "found": [9, 21], "respect": 9, "moreov": 9, "assumpt": [9, 15], "user": 9, "These": [9, 14], "allow": 9, "idea": [9, 11, 13], "wherea": 9, "other": [9, 12, 14, 21], "eum": [9, 16, 19], "mul": [9, 16, 19], "loss": [9, 16, 19], "problem": [9, 19], "know": [9, 14, 19], "equal": [9, 16], "simplex": [9, 16], "50": [9, 11, 16], "also": [9, 13, 16], "wandb": [9, 16, 21], "here": [9, 13, 21], "log_all_multi_policy_metr": [9, 16], "current_front": [9, 16], "hv_ref_point": [9, 16], "reward_dim": [9, 16, 20], "global_step": [9, 16], "n_sample_weight": [9, 16], "ref_front": [9, 16], "invert": [9, 16, 19], "approxim": [9, 16, 19], "global": [9, 16], "offici": 9, "sent": 9, "openrlbenchmark": [9, 21], "api": [9, 21], "queri": 9, "plot": 9, "format": [9, 21], "life": 9, "good": 9, "full": [9, 14], "flow": 9, "autom": 9, "cli": 9, "accordingli": 9, "locat": 9, "launch_experi": 9, "below": [9, 14], "issu": [9, 13], "predict": [9, 17], "hay": [9, 13], "et": [9, 16, 17], "al": [9, 16, 17], "practic": [9, 21], "plan": [9, 21], "36": 9, "apr": 9, "1007": 9, "s10458": 9, "022": 9, "09552": 9, "zintgraf": [9, 19], "t": [9, 19], "v": [9, 19], "kanter": [9, 19], "f": [9, 13, 19], "oliehoek": [9, 19], "beau": [9, 19], "approach": [9, 19], "2015": [9, 16, 17, 19], "accru": [11, 15], "reward": [11, 15, 20, 21], "futur": 11, "steckelmach": [11, 13], "2018": 11, "accrued_reward": [11, 14], "moqlearn": 12, "model_bas": 12, "tabular_model": 12, "tabularmodel": 12, "0001": [12, 16], "alpha": [12, 16], "parent": 12, "parent_writ": 12, "tensorboard": 12, "parent_rng": 12, "_gener": 12, "maintain": 12, "move": [12, 14], "scalarized_q_valu": 12, "500000": 12, "max": 12, "recal": 12, "launch": 12, "discord": 13, "server": 13, "where": 13, "you": 13, "ask": 13, "question": [13, 18], "help": 13, "repositori": [13, 21], "join": 13, "florian": [13, 21], "felten": [13, 21], "ffelten": 13, "lucasalegr": [13, 21], "open": [13, 21], "alwai": [13, 20], "happi": 13, "receiv": 13, "bug": 13, "fix": 13, "discuss": 13, "your": 13, "u": 13, "pull": 13, "request": 13, "directli": 13, "asid": 13, "contributor": 13, "mani": 13, "who": 13, "project": 13, "wai": [13, 14], "would": 13, "like": 13, "thank": 13, "willem": 13, "r\u00f6pke": 13, "hi": 13, "wilrop": 13, "deni": 13, "conor": 13, "librari": [14, 21], "replaybuff": 14, "obs_shap": 14, "action_dim": 14, "rew_dim": 14, "max_siz": 14, "obs_dtyp": 14, "float32": 14, "action_dtyp": 14, "next_ob": 14, "get_all_data": 14, "max_sampl": 14, "replac": 14, "use_c": 14, "to_tensor": 14, "batch": 14, "cer": 14, "convert": 14, "pytorch": [14, 21], "sample_ob": 14, "diverse_buff": 14, "diversememori": 14, "main_capac": 14, "sec_capac": 14, "trace_divers": 14, "crowding_divers": 14, "value_funct": 14, "integr": 14, "secondari": 14, "extract": 14, "axelabel": 14, "dynmorl": 14, "error": 14, "trace_id": 14, "pred_idx": 14, "tree_id": 14, "proport": 14, "treat": 14, "trace": 14, "identifi": 14, "tree": 14, "relev": 14, "index": 14, "node": 14, "wa": 14, "add_sampl": 14, "write": 14, "add_tre": 14, "dupe": 14, "trg_i": 14, "src_i": 14, "copi": 14, "sourc": [14, 18], "extract_trac": 14, "posit": 14, "those": 14, "get_data": 14, "include_indic": 14, "includ": 14, "get_error": 14, "idx": 14, "correspond": 14, "get_sec_writ": 14, "secondary_trac": 14, "reserved_idx": 14, "find": 14, "free": 14, "spot": 14, "memori": [14, 18], "recurs": 14, "past": 14, "low": 14, "crowd": 14, "get_trace_valu": 14, "trace_tupl": 14, "main_mem_is_ful": 14, "becaus": 14, "circular": 14, "suffici": 14, "move_to_sec": 14, "span": 14, "remove_trac": 14, "sec_dist": 14, "prioritized_buff": 14, "prioritizedreplaybuff": 14, "update_prior": 14, "accrued_reward_buff": 14, "accruedrewardreplaybuff": 14, "action_shap": 14, "cleanup": 14, "whole": 14, "order": 14, "element": [14, 16, 18], "relat": 15, "eval_mo": 15, "dot": [15, 19, 20], "render": [15, 16], "linearreward": 15, "wrapper": 15, "eval_mo_reward_condit": 15, "make": 15, "policy_evaluation_mo": 15, "rep": 15, "avg": 15, "equally_spaced_weight": 16, "dim": 16, "riesz": 16, "energi": 16, "pymoo": [16, 19], "misc": [16, 21], "reference_direct": 16, "extrema_weight": 16, "extrema": 16, "rest": 16, "get_grad_norm": 16, "param": 16, "how": 16, "grad": 16, "norm": 16, "insid": 16, "nn": [16, 17], "clip_grad_norm_": 16, "huber": 16, "minimum": 16, "layer_init": 16, "layer": [16, 17], "orthogon": 16, "weight_gain": 16, "bias_const": 16, "initi": 16, "gain": 16, "constant": 16, "bia": 16, "linearly_decaying_valu": 16, "initial_valu": 16, "decay_period": 16, "warmup_step": 16, "final_valu": 16, "linearli": 16, "decai": 16, "natur": [16, 17], "schedul": 16, "mnih": [16, 17], "begin": 16, "until": 16, "taken": 16, "period": 16, "complet": 16, "so": [16, 20], "far": [16, 20], "accord": 16, "log_episode_info": 16, "global_timestep": 16, "inform": 16, "last": 16, "automat": [16, 20, 21], "recordstatisticswrapp": 16, "statist": 16, "print": 16, "make_gif": 16, "fullpath": 16, "fp": 16, "length": [16, 17], "300": 16, "gif": 16, "polyak_upd": 16, "target_param": 16, "polyak": 16, "coeffici": 16, "usual": 16, "small": 16, "random_weight": 16, "dist": 16, "dirichlet": 16, "normal": [16, 17], "gaussian": 16, "distribut": 16, "equival": 16, "uniformli": 16, "seed_everyth": 16, "call": 16, "onli": [16, 19, 21], "onc": 16, "python": [16, 18], "prefer": 16, "script": 16, "effect": 16, "care": 16, "unique_tol": 16, "uniqu": 16, "within": 16, "naturecnn": 17, "observation_shap": 17, "features_dim": 17, "512": 17, "cnn": 17, "volodymyr": 17, "human": 17, "level": 17, "through": 17, "deep": 17, "518": 17, "7540": 17, "529": 17, "533": 17, "forward": 17, "mlp": 17, "input_dim": 17, "output_dim": 17, "activation_fn": 17, "modul": 17, "activ": 17, "relu": 17, "sequenti": 17, "perceptron": 17, "fulli": 17, "connect": 17, "dimens": [17, 20], "output": 17, "architectur": 17, "unit": 17, "dropout": 17, "rate": 17, "paretoarch": 18, "archiv": 18, "ineffici": 18, "get_non_domin": 18, "subset": 18, "stackoverflow": 18, "32791911": 18, "fast": 18, "answer": 18, "wrong": 18, "import": 18, "made": [18, 19], "get_non_dominated_ind": 18, "solut": 18, "boolean": 18, "mostli": 19, "axiomat": 19, "hv": 19, "customli": 19, "expected_util": 19, "weights_set": 19, "similar": 19, "But": 19, "need": 19, "assess": 19, "product": [19, 20], "_supportsarrai": 19, "dtype": 19, "_nestedsequ": 19, "complex": 19, "byte": 19, "known_front": 19, "current_estim": 19, "nearest": 19, "maximum_utility_loss": 19, "reference_set": 19, "basic": 19, "tchebicheff": 20, "seen": 20, "compon": 20, "sure": 20, "aim": 21, "reliabl": 21, "strictli": 21, "standard": 21, "mdp": 21, "momdp": 21, "suggest": 21, "read": 21, "under": 21, "criteria": 21, "report": 21, "bias": 21, "dashboard": 21, "lint": 21, "enforc": 21, "pre": 21, "commit": 21, "hook": 21, "well": 21, "etc": [21, 22], "against": 21, "ones": 21, "hyper": 21, "particip": 21, "popular": 21, "stabl": 21, "ai": 21, "titl": 21, "year": 21, "publish": 21, "howpublish": 21, "url": 21, "As": 22, "much": 22, "repo": 22, "tri": 22, "file": 22, "rule": 22, "structur": 22, "exampl": 22, "recur": 22, "concept": 22, "neural": 22}, "objects": {"morl_baselines.common.accrued_reward_buffer": [[14, 0, 1, "", "AccruedRewardReplayBuffer"]], "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer": [[14, 1, 1, "", "add"], [14, 1, 1, "", "cleanup"], [14, 1, 1, "", "get_all_data"], [14, 1, 1, "", "sample"]], "morl_baselines.common.buffer": [[14, 0, 1, "", "ReplayBuffer"]], "morl_baselines.common.buffer.ReplayBuffer": [[14, 1, 1, "", "add"], [14, 1, 1, "", "get_all_data"], [14, 1, 1, "", "sample"], [14, 1, 1, "", "sample_obs"]], "morl_baselines.common.diverse_buffer": [[14, 0, 1, "", "DiverseMemory"]], "morl_baselines.common.diverse_buffer.DiverseMemory": [[14, 1, 1, "", "add"], [14, 1, 1, "", "add_sample"], [14, 1, 1, "", "add_tree"], [14, 1, 1, "", "dupe"], [14, 1, 1, "", "extract_trace"], [14, 1, 1, "", "get"], [14, 1, 1, "", "get_data"], [14, 1, 1, "", "get_error"], [14, 1, 1, "", "get_sec_write"], [14, 1, 1, "", "get_trace_value"], [14, 1, 1, "", "main_mem_is_full"], [14, 1, 1, "", "move_to_sec"], [14, 1, 1, "", "remove_trace"], [14, 1, 1, "", "sample"], [14, 1, 1, "", "sec_distances"], [14, 1, 1, "", "update"]], "morl_baselines.common": [[15, 2, 0, "-", "evaluation"], [17, 2, 0, "-", "networks"], [18, 2, 0, "-", "pareto"], [19, 2, 0, "-", "performance_indicators"], [20, 2, 0, "-", "scalarization"], [16, 2, 0, "-", "utils"]], "morl_baselines.common.evaluation": [[15, 3, 1, "", "eval_mo"], [15, 3, 1, "", "eval_mo_reward_conditioned"], [15, 3, 1, "", "policy_evaluation_mo"]], "morl_baselines.common.networks": [[17, 0, 1, "", "NatureCNN"], [17, 3, 1, "", "mlp"]], "morl_baselines.common.networks.NatureCNN": [[17, 1, 1, "", "forward"]], "morl_baselines.common.pareto": [[18, 0, 1, "", "ParetoArchive"], [18, 3, 1, "", "get_non_dominated"], [18, 3, 1, "", "get_non_dominated_inds"]], "morl_baselines.common.pareto.ParetoArchive": [[18, 1, 1, "", "add"]], "morl_baselines.common.performance_indicators": [[19, 3, 1, "", "expected_utility"], [19, 3, 1, "", "hypervolume"], [19, 3, 1, "", "igd"], [19, 3, 1, "", "maximum_utility_loss"], [19, 3, 1, "", "sparsity"]], "morl_baselines.common.prioritized_buffer": [[14, 0, 1, "", "PrioritizedReplayBuffer"]], "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer": [[14, 1, 1, "", "add"], [14, 1, 1, "", "get_all_data"], [14, 1, 1, "", "sample"], [14, 1, 1, "", "sample_obs"], [14, 1, 1, "", "update_priorities"]], "morl_baselines.common.scalarization": [[20, 3, 1, "", "tchebicheff"], [20, 3, 1, "", "weighted_sum"]], "morl_baselines.common.utils": [[16, 3, 1, "", "equally_spaced_weights"], [16, 3, 1, "", "extrema_weights"], [16, 3, 1, "", "get_grad_norm"], [16, 3, 1, "", "huber"], [16, 3, 1, "", "layer_init"], [16, 3, 1, "", "linearly_decaying_value"], [16, 3, 1, "", "log_all_multi_policy_metrics"], [16, 3, 1, "", "log_episode_info"], [16, 3, 1, "", "make_gif"], [16, 3, 1, "", "polyak_update"], [16, 3, 1, "", "random_weights"], [16, 3, 1, "", "seed_everything"], [16, 3, 1, "", "unique_tol"]], "morl_baselines.multi_policy.envelope.envelope": [[2, 0, 1, "", "Envelope"]], "morl_baselines.multi_policy.envelope.envelope.Envelope": [[2, 1, 1, "", "act"], [2, 1, 1, "", "ddqn_target"], [2, 1, 1, "", "envelope_target"], [2, 1, 1, "", "eval"], [2, 1, 1, "", "get_config"], [2, 1, 1, "", "load"], [2, 1, 1, "", "max_action"], [2, 1, 1, "", "save"], [2, 1, 1, "", "train"], [2, 1, 1, "", "update"]], "morl_baselines.multi_policy.gpi_pd.gpi_pd": [[3, 0, 1, "", "GPIPD"]], "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD": [[3, 1, 1, "", "eval"], [3, 1, 1, "", "get_config"], [3, 1, 1, "", "gpi_action"], [3, 1, 1, "", "load"], [3, 1, 1, "", "max_action"], [3, 1, 1, "", "save"], [3, 1, 1, "", "set_weight_support"], [3, 1, 1, "", "train"], [3, 1, 1, "", "train_iteration"], [3, 1, 1, "", "update"]], "morl_baselines.multi_policy.linear_support.linear_support": [[4, 0, 1, "", "LinearSupport"]], "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport": [[4, 1, 1, "", "add_solution"], [4, 1, 1, "", "compute_corner_weights"], [4, 1, 1, "", "ended"], [4, 1, 1, "", "get_corner_weights"], [4, 1, 1, "", "get_weight_support"], [4, 1, 1, "", "gpi_ls_priority"], [4, 1, 1, "", "is_dominated"], [4, 1, 1, "", "max_scalarized_value"], [4, 1, 1, "", "max_value_lp"], [4, 1, 1, "", "next_weight"], [4, 1, 1, "", "ols_priority"], [4, 1, 1, "", "remove_obsolete_values"], [4, 1, 1, "", "remove_obsolete_weights"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning": [[5, 0, 1, "", "MPMOQLearning"]], "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning": [[5, 1, 1, "", "delete_policies"], [5, 1, 1, "", "eval"], [5, 1, 1, "", "get_config"], [5, 1, 1, "", "max_scalar_q_value"], [5, 1, 1, "", "train"]], "morl_baselines.multi_policy.pareto_q_learning.pql": [[6, 0, 1, "", "PQL"]], "morl_baselines.multi_policy.pareto_q_learning.pql.PQL": [[6, 1, 1, "", "calc_non_dominated"], [6, 1, 1, "", "get_config"], [6, 1, 1, "", "get_local_pcs"], [6, 1, 1, "", "get_q_set"], [6, 1, 1, "", "score_hypervolume"], [6, 1, 1, "", "score_pareto_cardinality"], [6, 1, 1, "", "select_action"], [6, 1, 1, "", "track_policy"], [6, 1, 1, "", "train"]], "morl_baselines.multi_policy.pcn.pcn": [[7, 0, 1, "", "PCN"]], "morl_baselines.multi_policy.pcn.pcn.PCN": [[7, 1, 1, "", "eval"], [7, 1, 1, "", "evaluate"], [7, 1, 1, "", "get_config"], [7, 1, 1, "", "save"], [7, 1, 1, "", "set_desired_return_and_horizon"], [7, 1, 1, "", "train"], [7, 1, 1, "", "update"]], "morl_baselines.multi_policy.pgmorl.pgmorl": [[8, 0, 1, "", "PGMORL"], [8, 0, 1, "", "PerformancePredictor"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL": [[8, 1, 1, "", "get_config"], [8, 1, 1, "", "train"]], "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor": [[8, 1, 1, "", "add"], [8, 1, 1, "", "predict_next_evaluation"]], "morl_baselines.single_policy.esr.eupg": [[11, 0, 1, "", "EUPG"]], "morl_baselines.single_policy.esr.eupg.EUPG": [[11, 1, 1, "", "eval"], [11, 1, 1, "", "get_config"], [11, 1, 1, "", "train"], [11, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_ppo": [[8, 0, 1, "", "MOPPO"]], "morl_baselines.single_policy.ser.mo_ppo.MOPPO": [[8, 1, 1, "", "change_weights"], [8, 1, 1, "", "eval"], [8, 1, 1, "", "train"], [8, 1, 1, "", "update"]], "morl_baselines.single_policy.ser.mo_q_learning": [[12, 0, 1, "", "MOQLearning"]], "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning": [[12, 1, 1, "", "eval"], [12, 1, 1, "", "get_config"], [12, 1, 1, "", "scalarized_q_values"], [12, 1, 1, "", "train"], [12, 1, 1, "", "update"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:module", "3": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "module", "Python module"], "3": ["py", "function", "Python function"]}, "titleterms": {"overview": [0, 22], "multi": [1, 9, 14, 21], "polici": [1, 9, 10], "algorithm": [1, 9, 10, 21], "envelop": 2, "q": [2, 6], "learn": [2, 5, 6, 12, 21], "gpi": 3, "priorit": [3, 14], "dyna": 3, "linear": 4, "support": 4, "mpmoq": 5, "pareto": [6, 7, 18], "condit": 7, "network": [7, 17], "pgmorl": 8, "applic": 8, "limit": 8, "principl": 8, "moppo": 8, "weight": 8, "gener": 8, "predict": 8, "model": 8, "perform": [9, 19], "assess": 9, "introduct": 9, "metric": 9, "singl": [9, 10], "storag": 9, "benchmark": [9, 21], "script": 9, "refer": 9, "eupg": 11, "moq": 12, "commun": 13, "maintain": 13, "contribut": 13, "acknowledg": 13, "replai": 14, "buffer": 14, "object": [14, 21], "divers": 14, "accru": 14, "reward": 14, "evalu": 15, "miscellan": 16, "neural": 17, "helper": 17, "util": 18, "indic": 19, "scalar": 20, "function": 20, "morl": 21, "baselin": 21, "A": 21, "collect": 21, "reinforc": 21, "featur": 21, "cite": 21}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx": 57}, "alltitles": {"Overview": [[0, "overview"], [22, "overview"]], "Multi-Policy Algorithms": [[1, "multi-policy-algorithms"]], "Envelope Q-Learning": [[2, "envelope-q-learning"]], "GPI-Prioritized Dyna": [[3, "gpi-prioritized-dyna"]], "Linear Support": [[4, "linear-support"]], "MPMOQ Learning": [[5, "mpmoq-learning"]], "Pareto Q-Learning": [[6, "pareto-q-learning"]], "Pareto Conditioned Networks": [[7, "pareto-conditioned-networks"]], "PGMORL": [[8, "pgmorl"], [8, "id1"]], "Applicability and limitations": [[8, "applicability-and-limitations"]], "Principle": [[8, "principle"]], "MOPPO": [[8, "moppo"]], "Weight generator - prediction model": [[8, "weight-generator-prediction-model"]], "Performance assessments": [[9, "performance-assessments"]], "Introduction": [[9, "introduction"]], "Metrics": [[9, "metrics"]], "Single-policy algorithms": [[9, "single-policy-algorithms"]], "Multi-policy algorithms": [[9, "multi-policy-algorithms"]], "Storage": [[9, "storage"]], "Benchmarking script": [[9, "benchmarking-script"]], "Algorithms": [[9, "algorithms"]], "References": [[9, "references"]], "Single-policy Algorithms": [[10, "single-policy-algorithms"]], "EUPG": [[11, "eupg"]], "MOQ-Learning": [[12, "moq-learning"]], "Community": [[13, "community"]], "Maintainers": [[13, "maintainers"]], "Contributing": [[13, "contributing"]], "Acknowledgements": [[13, "acknowledgements"]], "Replay Buffers": [[14, "replay-buffers"]], "Multi-Objective Replay Buffer": [[14, "multi-objective-replay-buffer"]], "Diverse Replay Buffer": [[14, "diverse-replay-buffer"]], "Prioritized Replay Buffer": [[14, "prioritized-replay-buffer"]], "Accrued Reward Replay Buffer": [[14, "accrued-reward-replay-buffer"]], "Evaluations": [[15, "module-morl_baselines.common.evaluation"]], "Miscellaneous": [[16, "module-morl_baselines.common.utils"]], "Neural Networks helpers": [[17, "module-morl_baselines.common.networks"]], "Pareto utils": [[18, "module-morl_baselines.common.pareto"]], "Performance indicators": [[19, "module-morl_baselines.common.performance_indicators"]], "Scalarization functions": [[20, "module-morl_baselines.common.scalarization"]], "MORL-Baselines: A collection of multi-objective reinforcement learning algorithms.": [[21, "morl-baselines-a-collection-of-multi-objective-reinforcement-learning-algorithms"]], "Features of MORL-Baselines": [[21, "features-of-morl-baselines"]], "Benchmarks": [[21, "benchmarks"]], "Citing MORL-Baselines": [[21, "citing-morl-baselines"]]}, "indexentries": {"envelope (class in morl_baselines.multi_policy.envelope.envelope)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope"]], "act() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.act"]], "ddqn_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.ddqn_target"]], "envelope_target() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.envelope_target"]], "eval() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.eval"]], "get_config() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.get_config"]], "load() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.load"]], "max_action() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.max_action"]], "save() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.save"]], "train() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.train"]], "update() (morl_baselines.multi_policy.envelope.envelope.envelope method)": [[2, "morl_baselines.multi_policy.envelope.envelope.Envelope.update"]], "gpipd (class in morl_baselines.multi_policy.gpi_pd.gpi_pd)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD"]], "eval() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.eval"]], "get_config() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.get_config"]], "gpi_action() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.gpi_action"]], "load() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.load"]], "max_action() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.max_action"]], "save() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.save"]], "set_weight_support() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.set_weight_support"]], "train() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.train"]], "train_iteration() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.train_iteration"]], "update() (morl_baselines.multi_policy.gpi_pd.gpi_pd.gpipd method)": [[3, "morl_baselines.multi_policy.gpi_pd.gpi_pd.GPIPD.update"]], "linearsupport (class in morl_baselines.multi_policy.linear_support.linear_support)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport"]], "add_solution() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.add_solution"]], "compute_corner_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.compute_corner_weights"]], "ended() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.ended"]], "get_corner_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.get_corner_weights"]], "get_weight_support() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.get_weight_support"]], "gpi_ls_priority() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.gpi_ls_priority"]], "is_dominated() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.is_dominated"]], "max_scalarized_value() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.max_scalarized_value"]], "max_value_lp() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.max_value_lp"]], "next_weight() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.next_weight"]], "ols_priority() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.ols_priority"]], "remove_obsolete_values() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.remove_obsolete_values"]], "remove_obsolete_weights() (morl_baselines.multi_policy.linear_support.linear_support.linearsupport method)": [[4, "morl_baselines.multi_policy.linear_support.linear_support.LinearSupport.remove_obsolete_weights"]], "mpmoqlearning (class in morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning"]], "delete_policies() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.delete_policies"]], "eval() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.eval"]], "get_config() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.get_config"]], "max_scalar_q_value() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.max_scalar_q_value"]], "train() (morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.mpmoqlearning method)": [[5, "morl_baselines.multi_policy.multi_policy_moqlearning.mp_mo_q_learning.MPMOQLearning.train"]], "pql (class in morl_baselines.multi_policy.pareto_q_learning.pql)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL"]], "calc_non_dominated() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.calc_non_dominated"]], "get_config() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_config"]], "get_local_pcs() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_local_pcs"]], "get_q_set() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.get_q_set"]], "score_hypervolume() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_hypervolume"]], "score_pareto_cardinality() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.score_pareto_cardinality"]], "select_action() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.select_action"]], "track_policy() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.track_policy"]], "train() (morl_baselines.multi_policy.pareto_q_learning.pql.pql method)": [[6, "morl_baselines.multi_policy.pareto_q_learning.pql.PQL.train"]], "pcn (class in morl_baselines.multi_policy.pcn.pcn)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN"]], "eval() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.eval"]], "evaluate() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.evaluate"]], "get_config() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.get_config"]], "save() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.save"]], "set_desired_return_and_horizon() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.set_desired_return_and_horizon"]], "train() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.train"]], "update() (morl_baselines.multi_policy.pcn.pcn.pcn method)": [[7, "morl_baselines.multi_policy.pcn.pcn.PCN.update"]], "moppo (class in morl_baselines.single_policy.ser.mo_ppo)": [[8, "morl_baselines.single_policy.ser.mo_ppo.MOPPO"]], "pgmorl (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL"]], "performancepredictor (class in morl_baselines.multi_policy.pgmorl.pgmorl)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor"]], "add() (morl_baselines.multi_policy.pgmorl.pgmorl.performancepredictor method)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor.add"]], "change_weights() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[8, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.change_weights"]], "eval() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[8, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.eval"]], "get_config() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.get_config"]], "predict_next_evaluation() (morl_baselines.multi_policy.pgmorl.pgmorl.performancepredictor method)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PerformancePredictor.predict_next_evaluation"]], "train() (morl_baselines.multi_policy.pgmorl.pgmorl.pgmorl method)": [[8, "morl_baselines.multi_policy.pgmorl.pgmorl.PGMORL.train"]], "train() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[8, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.train"]], "update() (morl_baselines.single_policy.ser.mo_ppo.moppo method)": [[8, "morl_baselines.single_policy.ser.mo_ppo.MOPPO.update"]], "log_all_multi_policy_metrics() (in module morl_baselines.common.utils)": [[9, "morl_baselines.common.utils.log_all_multi_policy_metrics"], [16, "morl_baselines.common.utils.log_all_multi_policy_metrics"]], "eupg (class in morl_baselines.single_policy.esr.eupg)": [[11, "morl_baselines.single_policy.esr.eupg.EUPG"]], "eval() (morl_baselines.single_policy.esr.eupg.eupg method)": [[11, "morl_baselines.single_policy.esr.eupg.EUPG.eval"]], "get_config() (morl_baselines.single_policy.esr.eupg.eupg method)": [[11, "morl_baselines.single_policy.esr.eupg.EUPG.get_config"]], "train() (morl_baselines.single_policy.esr.eupg.eupg method)": [[11, "morl_baselines.single_policy.esr.eupg.EUPG.train"]], "update() (morl_baselines.single_policy.esr.eupg.eupg method)": [[11, "morl_baselines.single_policy.esr.eupg.EUPG.update"]], "moqlearning (class in morl_baselines.single_policy.ser.mo_q_learning)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning"]], "eval() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.eval"]], "get_config() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.get_config"]], "scalarized_q_values() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.scalarized_q_values"]], "train() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.train"]], "update() (morl_baselines.single_policy.ser.mo_q_learning.moqlearning method)": [[12, "morl_baselines.single_policy.ser.mo_q_learning.MOQLearning.update"]], "accruedrewardreplaybuffer (class in morl_baselines.common.accrued_reward_buffer)": [[14, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer"]], "diversememory (class in morl_baselines.common.diverse_buffer)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory"]], "prioritizedreplaybuffer (class in morl_baselines.common.prioritized_buffer)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer"]], "replaybuffer (class in morl_baselines.common.buffer)": [[14, "morl_baselines.common.buffer.ReplayBuffer"]], "add() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[14, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.add"]], "add() (morl_baselines.common.buffer.replaybuffer method)": [[14, "morl_baselines.common.buffer.ReplayBuffer.add"]], "add() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.add"]], "add() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.add"]], "add_sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.add_sample"]], "add_tree() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.add_tree"]], "cleanup() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[14, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.cleanup"]], "dupe() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.dupe"]], "extract_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.extract_trace"]], "get() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.get"]], "get_all_data() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[14, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.buffer.replaybuffer method)": [[14, "morl_baselines.common.buffer.ReplayBuffer.get_all_data"]], "get_all_data() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.get_all_data"]], "get_data() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.get_data"]], "get_error() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.get_error"]], "get_sec_write() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.get_sec_write"]], "get_trace_value() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.get_trace_value"]], "main_mem_is_full() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.main_mem_is_full"]], "move_to_sec() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.move_to_sec"]], "remove_trace() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.remove_trace"]], "sample() (morl_baselines.common.accrued_reward_buffer.accruedrewardreplaybuffer method)": [[14, "morl_baselines.common.accrued_reward_buffer.AccruedRewardReplayBuffer.sample"]], "sample() (morl_baselines.common.buffer.replaybuffer method)": [[14, "morl_baselines.common.buffer.ReplayBuffer.sample"]], "sample() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.sample"]], "sample() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample"]], "sample_obs() (morl_baselines.common.buffer.replaybuffer method)": [[14, "morl_baselines.common.buffer.ReplayBuffer.sample_obs"]], "sample_obs() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.sample_obs"]], "sec_distances() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.sec_distances"]], "update() (morl_baselines.common.diverse_buffer.diversememory method)": [[14, "morl_baselines.common.diverse_buffer.DiverseMemory.update"]], "update_priorities() (morl_baselines.common.prioritized_buffer.prioritizedreplaybuffer method)": [[14, "morl_baselines.common.prioritized_buffer.PrioritizedReplayBuffer.update_priorities"]], "eval_mo() (in module morl_baselines.common.evaluation)": [[15, "morl_baselines.common.evaluation.eval_mo"]], "eval_mo_reward_conditioned() (in module morl_baselines.common.evaluation)": [[15, "morl_baselines.common.evaluation.eval_mo_reward_conditioned"]], "module": [[15, "module-morl_baselines.common.evaluation"], [16, "module-morl_baselines.common.utils"], [17, "module-morl_baselines.common.networks"], [18, "module-morl_baselines.common.pareto"], [19, "module-morl_baselines.common.performance_indicators"], [20, "module-morl_baselines.common.scalarization"]], "morl_baselines.common.evaluation": [[15, "module-morl_baselines.common.evaluation"]], "policy_evaluation_mo() (in module morl_baselines.common.evaluation)": [[15, "morl_baselines.common.evaluation.policy_evaluation_mo"]], "equally_spaced_weights() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.equally_spaced_weights"]], "extrema_weights() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.extrema_weights"]], "get_grad_norm() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.get_grad_norm"]], "huber() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.huber"]], "layer_init() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.layer_init"]], "linearly_decaying_value() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.linearly_decaying_value"]], "log_episode_info() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.log_episode_info"]], "make_gif() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.make_gif"]], "morl_baselines.common.utils": [[16, "module-morl_baselines.common.utils"]], "polyak_update() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.polyak_update"]], "random_weights() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.random_weights"]], "seed_everything() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.seed_everything"]], "unique_tol() (in module morl_baselines.common.utils)": [[16, "morl_baselines.common.utils.unique_tol"]], "naturecnn (class in morl_baselines.common.networks)": [[17, "morl_baselines.common.networks.NatureCNN"]], "forward() (morl_baselines.common.networks.naturecnn method)": [[17, "morl_baselines.common.networks.NatureCNN.forward"]], "mlp() (in module morl_baselines.common.networks)": [[17, "morl_baselines.common.networks.mlp"]], "morl_baselines.common.networks": [[17, "module-morl_baselines.common.networks"]], "paretoarchive (class in morl_baselines.common.pareto)": [[18, "morl_baselines.common.pareto.ParetoArchive"]], "add() (morl_baselines.common.pareto.paretoarchive method)": [[18, "morl_baselines.common.pareto.ParetoArchive.add"]], "get_non_dominated() (in module morl_baselines.common.pareto)": [[18, "morl_baselines.common.pareto.get_non_dominated"]], "get_non_dominated_inds() (in module morl_baselines.common.pareto)": [[18, "morl_baselines.common.pareto.get_non_dominated_inds"]], "morl_baselines.common.pareto": [[18, "module-morl_baselines.common.pareto"]], "expected_utility() (in module morl_baselines.common.performance_indicators)": [[19, "morl_baselines.common.performance_indicators.expected_utility"]], "hypervolume() (in module morl_baselines.common.performance_indicators)": [[19, "morl_baselines.common.performance_indicators.hypervolume"]], "igd() (in module morl_baselines.common.performance_indicators)": [[19, "morl_baselines.common.performance_indicators.igd"]], "maximum_utility_loss() (in module morl_baselines.common.performance_indicators)": [[19, "morl_baselines.common.performance_indicators.maximum_utility_loss"]], "morl_baselines.common.performance_indicators": [[19, "module-morl_baselines.common.performance_indicators"]], "sparsity() (in module morl_baselines.common.performance_indicators)": [[19, "morl_baselines.common.performance_indicators.sparsity"]], "morl_baselines.common.scalarization": [[20, "module-morl_baselines.common.scalarization"]], "tchebicheff() (in module morl_baselines.common.scalarization)": [[20, "morl_baselines.common.scalarization.tchebicheff"]], "weighted_sum() (in module morl_baselines.common.scalarization)": [[20, "morl_baselines.common.scalarization.weighted_sum"]]}})